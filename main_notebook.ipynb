{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark-RuneScape\n",
    "Solução simples para aprendizado com PySpark, onde faço a ingestão de dados da API de monstros do jogo RuneScape, realizo algumas operações simples (filtragem e agregação) e converto os dados para o formato Parquet.\n",
    "\n",
    "# Como utilizar\n",
    "\n",
    "O script deverá ser rodado em ambiente Ubuntu 22.04.\n",
    "\n",
    "1. Realizar a instalação de Python versão 3.10.12\n",
    "2. Realização da instalação de Java JDK versão 11.0.22\n",
    "3. Instalar as bibliotecas do requirements.txt\n",
    "4. Executar o script \"main.py\" na pasta raíz do repositório.\n",
    "\n",
    "O script gerará arquivos de input, printará as filtragens e agregações no dataframe, e criará os outputs parquet em pastas dentro do próprio repositório.\n",
    "\n",
    "# Primeiros passos\n",
    "\n",
    "Declaração de variáveis de ambiente que serão utilizadas no processo:\n",
    "\n",
    "* JSON_DATA_PATH: Caminho onde os arquivos que serão consumidos da API serão armazenados (source);\n",
    "* PARQUET_DATA_PATH: Caminho onde os arquivos serão armazenados após o processo de transformação (target);\n",
    "* RUNESCAPE_BASE_URL: Link da API onde serão capturados os arquivos que serão ingeridos em formato JSON, transformados e armazenados em Parquet;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_DATA_PATH = \"./Data/Json\"\n",
    "PARQUET_DATA_PATH = \"./Data/Parquet\"\n",
    "RUNESCAPE_BASE_URL = \"https://secure.runescape.com/m=itemdb_rs/bestiary/beastData.json?beastid=\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação as libs necessárias para execução do processo, principalmente as funções e dependências do pyspark;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from faker import Faker\n",
    "from json import dump\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaração da classe personalizada do Spark para setar as variáveis necessárias para o processo;\n",
    "Criação de pastas necessárias para as transações caso não existam;\n",
    "Criação da sessão do Spark;\n",
    "Declaração de funções para manipualação dos dados:\n",
    "\n",
    "- read_source_data: função para leitura do arquivo JSON;\n",
    "- mfilter_data: funçao para filtragem do DataFrame onde só são considerados arquivos onde o campo member = True;\n",
    "- group_by_count: Agrupamento para contabilização de membros e não membros;\n",
    "- order_by_release: Função orgenação do Data Frame pela data \"date_released\";\n",
    "- convert_to_parquet: função para salvar os dados em formato parquet no diretório padrão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PySparkRuneScape():\n",
    "    def __init__(self):\n",
    "        # Instancia caminhos de dados\n",
    "        self.source_data_path = JSON_DATA_PATH\n",
    "        self.destination_path = PARQUET_DATA_PATH\n",
    "    \n",
    "        # Verifica se PARQUET_DATA_PATH existe e cria, caso não exista\n",
    "        if not os.path.isdir(PARQUET_DATA_PATH):\n",
    "            os.makedirs(PARQUET_DATA_PATH)\n",
    "\n",
    "        # Inicia sessão Spark\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"RuneScapeStream\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        self.source_data = self.read_source_data()\n",
    "    \n",
    "    def read_source_data(self):\n",
    "        return self.spark.read.json(self.source_data_path)\n",
    "    \n",
    "    def filter_data(self, df, column: str, value: str):\n",
    "        return df.where(col(column) == value)\n",
    "\n",
    "    def group_by_count(self, df, column: str):\n",
    "        return df.groupBy(column).count()\n",
    "\n",
    "    def order_by_release(self, df):\n",
    "        window_spec = Window.orderBy(\"date_released\")\n",
    "        return df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "    def convert_to_parquet(self, df):\n",
    "        df.write.parquet(self.destination_path, mode=\"overwrite\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaração de função para limpeza, caso necessário, dos diretório onde são armazenados os arquivos de entrada no formato JSON;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erase_input_files():\n",
    "    for file in os.listdir(JSON_DATA_PATH):\n",
    "        os.remove(os.path.join(JSON_DATA_PATH, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaração de função para realização das requisições na API através de ids sequenciais com increment de 1;\n",
    "Salvamento os arquivos JSON nomeando-os pelo id de membro;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_runescape_data(num_of_beasts):  \n",
    "    # Instanciamento de classe fabricadora de dados\n",
    "    fake = Faker()\n",
    "    \n",
    "    # Cria diretórios se houver necessidade\n",
    "    if not os.path.exists(JSON_DATA_PATH):\n",
    "        os.makedirs(JSON_DATA_PATH)\n",
    "    \n",
    "    for id in range(1, num_of_beasts + 1):\n",
    "        \n",
    "        beast_data = requests.get(RUNESCAPE_BASE_URL + str(id))\n",
    "        if beast_data.status_code == 200:\n",
    "            if beast_data.text != '':\n",
    "                \n",
    "                beast_data = beast_data.json()\n",
    "\n",
    "                # Gera dados fake de data para utilizar função window\n",
    "                beast_data['date_released'] = fake.date()\n",
    "                \n",
    "                with open(f\"{JSON_DATA_PATH}/{id}.json\", 'w') as file:\n",
    "                    dump(beast_data, fp=file)\n",
    "            \n",
    "            else:\n",
    "                print(f\"Beast data for ID {id} returned empty. Status code: {beast_data.status_code}\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for beast with ID {id}. Status code: {beast_data.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Alimentação do argumento de quantidade de função get_runescape_data para definição do número de membros que serão capturados;\n",
    "* Atribui a instancia da classe de pyspark a uma variável;\n",
    "* Cria uma Data Frame e filtra apenas onde o campo \"member\" é igual a \"true\";\n",
    "* Exibe os resultados do Data Frame;\n",
    "* Cria uma novo Data Frame, usando o anterior, ordernado pela data \"released_date\";\n",
    "* Exibe os resultados do Data Frame;\n",
    "* Cria uma novo Data Frame,usando o anterior, agrupando pelo campo \"members\" e realizando uma contagem;\n",
    "* Converte os dados para Parquet e salva no diretório já especificado em passos anteriores;\n",
    "* Apaga os arquivos no formato JSON utilizados para ingestão inicial;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pega dados fonte\n",
    "get_runescape_data(10)\n",
    "\n",
    "# Instancia classe de pyspark\n",
    "rs = PySparkRuneScape()\n",
    "\n",
    "# Filtra monstros para \"membros\"\n",
    "members_monsters = rs.filter_data(rs.source_data, \"members\", \"true\")\n",
    "members_monsters.show()\n",
    "\n",
    "# Ordena monstros para \"membros\" por released_date\n",
    "members_monsters_ordered = rs.order_by_release(members_monsters)\n",
    "members_monsters_ordered.show()\n",
    "\n",
    "# Conta quantos monstros são para \"membros\" e quantos não\n",
    "members_monsters_count = rs.group_by_count(rs.source_data, \"members\")\n",
    "members_monsters_count.show()\n",
    "\n",
    "# Converte dados em JSON para Parquet\n",
    "rs.convert_to_parquet(rs.source_data)\n",
    "\n",
    "# Apaga arquivos fonte\n",
    "erase_input_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
